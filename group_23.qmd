---
title: "ADD TITLE"
number-sections: true
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: default
editor_options: 
  chunk_output_type: console
execute:
  eval: true
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(janitor)
library(ggplot2)
library(moderndive)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(GGally)
library(gt)
library(pROC)
library(randomForest)
library(caret)
```

# Introduction

```{r}
data<-read.csv('D:/desktop/dataset23.csv') 
data$yesno<-as.factor(data$yesno) 
data <- data[rowSums(data[, 2:6] > 1) == 0, ] # the percentage of total numbe can not be greater than 1
```

```{r}
#| label: table 1
#| tbl-cap: summary of mean
data |>
  summarize(
    crl.tot = mean(crl.tot),
    dollar = mean(dollar),
    bang = mean(bang),
    money = mean(money),
    n000 = mean(n000),
    make = mean(make),
            .by = yesno) |>
  gt() |>
  fmt_number(decimals=2)
```

```{r}
#| label: table 2
#| tbl-cap: summary of median
data |>
  summarize(
    crl.tot = median(crl.tot),
    dollar = median(dollar),
    bang = median(bang),
    money = median(money),
    n000 = median(n000),
    make = median(make),
    .by = yesno) |>
  gt() |>
  fmt_number(decimals=2)

```

#Most mean values greater than median values may indicate right skewness.

```{r}
cor_matrix <- cor(data[, c("crl.tot", "dollar", "bang", "money", "n000", "make")])
corrplot::corrplot(cor_matrix, method = "number") 
```

#correlation

```{r}
ggplot(data, aes(x=yesno, y=crl.tot, fill=yesno)) +
  geom_boxplot() +
  labs(title="crl.tot by Class")
ggplot(data, aes(x=crl.tot, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="crl.tot Density by Class")
```

```{r}
ggplot(data, aes(x=yesno, y=bang, fill=yesno)) +
  geom_boxplot() +
  labs(title="bang by Class")
ggplot(data, aes(x=bang, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="bang Density by Class")
```

```{r}
ggplot(data, aes(x=yesno, y=money, fill=yesno)) +
  geom_boxplot() +
  labs(title="money by Class")
ggplot(data, aes(x=money, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="money Density by Class")
```

```{r}
ggplot(data, aes(x=yesno, y=dollar, fill=yesno)) +
  geom_boxplot() +
  labs(title="dollar by Class")
ggplot(data, aes(x=dollar, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="dollar Density by Class")
```

```{r}
ggplot(data, aes(x=yesno, y=n000, fill=yesno)) +
  geom_boxplot() +
  labs(title="n000 by Class")
ggplot(data, aes(x=n000, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="n000 Density by Class")
```

```{r}
ggplot(data, aes(x=yesno, y=make, fill=yesno)) +
  geom_boxplot() +
  labs(title="make by Class")
ggplot(data, aes(x=make, fill=yesno)) +
  geom_density(alpha=0.5) +
  labs(title="make Density by Class")
```

# Modelling
In the modeling section, data preprocessing was first conducted, including winsorization (outlier removal), standardization, and logarithmic transformation. Subsequently, three logistic regression models were constructed: one using the original data, one with standardized variables, and another incorporating log-transformed variables. The Akaike Information Criterion (AIC) was employed to compare model performance. Finally, regression coefficients and predictive outcomes were visualized. The ultimate objective was to identify the optimal transformation of predictor variables to enhance model performance.

## Data Preprocessing
Replace values below the 1st percentile with the 1st percentile value and values over the 99th percentile with the 99th percentile value, then standardize the data to mitigate the effects of outliers and right skewness. Due to the wide distribution and right-skewed nature of crl.tot, we substitute it with log(data\$crl.tot + 1).
```{r}
data1<-data

# Winsorization function to limit extreme values
win <- function(x, lower_perc = 0.01, upper_perc = 0.99) {
  x <- as.numeric(x)
  q <- quantile(x, probs = c(lower_perc, upper_perc), na.rm = TRUE)
  x[x < q[1]] <- q[1]
  x[x > q[2]] <- q[2]
  return(x)
}

# Define numeric variables and apply winsorization
numeric_vars <- c("crl.tot", "dollar", "bang", "money", "n000", "make")
data[numeric_vars] <- lapply(data[numeric_vars], win)

# Log transformation of crl.tot
data[,1:6]<-scale(data[,1:6])

# Log transformation of crl.tot
data$crl.tot_log <- log(data$crl.tot+1)

```

$$Y_i \sim \mathrm{Bernoulli}(p_i)$$

$$\quad \log\left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 \text{crl.tot}_i + \beta_2 \text{dollar}_i + \beta_3 \text{bang}_i + \beta_4 \text{money}_i + \beta_5 \text{n000}_i + \beta_6 \text{make}_i$$

## Variable Definitions
- **\(Y_i\)**: A binary factor variable indicating if the email was spam (`y`) or not (`n`).
- **\(\text{crl.tot}_i\)**: Total length of uninterrupted sequences of capital letters.
- **\(\text{dollar}_i\)**: Occurrences of the dollar sign (`$`), as a percentage of total number of characters.
- **\(\text{bang}_i\)**: Occurrences of `!`, as a percentage of total number of characters.
- **\(\text{money}_i\)**: Occurrences of the word "money", as a percentage of total number of characters.
- **\(\text{n000}_i\)**: Occurrences of the string "000", as a percentage of total number of characters.
- **\(\text{make}_i\)**: Occurrences of the word "make", as a percentage of total number of characters.

## Alternative Model with Log Transformation

An alternative model is estimated with a log transformation of `crl.tot`:
$$
\log\left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 \log(\text{crl.tot}_i) + \beta_2 \text{dollar}_i + \beta_3 \text{bang}_i + \beta_4 \text{money}_i + \beta_5 \text{n000}_i + \beta_6 \text{make}_i
$$

## Logistic regression models
```{r}
model_original <- glm(yesno ~ crl.tot + dollar + bang + money + n000 + make,
             family = binomial(link = "logit"),
             data = data1)

summary(model_original)

model_scale <- glm(yesno ~ crl.tot + dollar + bang + money + n000 + make,
             family = binomial(link = "logit"),
             data = data)

summary(model_scale)

model_scale_log <- glm(yesno ~ bang + crl.tot_log + dollar+money+n000+make,
               family = binomial(link = "logit"), data = data)
summary(model_scale_log)
```

## Model Comparison using AIC
```{r}
AIC(model_original,model_scale,model_scale_log)
```
The Akaike Information Criterion (AIC) values indicate the performance of the three logistic regression models:
- model_original: AIC = 592.74
- model_scale: AIC = 589.17 (improved after standardization)
- model_scale_log: AIC = 579.11 (best-performing model with log transformation)

Since a lower AIC value indicates a better fit, the log-transformed model (`model_scale_log`) is the optimal choice, as it achieves the lowest AIC.

## Model Visualization
```{r}
plot_model(model_scale_log, show.values = TRUE, show.p = TRUE)
```
The coefficient plot displays the estimated odds ratios:
Significant Positive Effects: `bang` (3.11), `crl.tot_log` (1.93), `dollar` (2.32), `money` (2.03), and `n000` (3.17) all have positive effects on the probability of an email being spam. This means that higher occurrences of these features increase the likelihood of spam classification.
Non-Significant / Negative Effect: `make` (0.86) has a negative effect and is not statistically significant, meaning it does not contribute to predicting spam effectively.

## Predicted probabilities
```{r}
plot_model(model_scale_log, type = "pred", title = "",col='steelblue')
```
The coefficient plot displays the estimated odds ratios:
Significant Positive Effects: `bang` (3.11), `crl.tot_log` (1.93), `dollar` (2.32), `money` (2.03), and `n000` (3.17) all have positive effects on the probability of an email being spam. This means that higher occurrences of these features increase the likelihood of spam classification.
Non-Significant / Negative Effect: `make` (0.86) has a negative effect and is not statistically significant, meaning it does not contribute to predicting spam effectively.

# Conclision
The analysis demonstrates that log transformation of `crl.tot` improves model performance, as indicated by the lowest AIC value in `model_scale_log`. Key predictors of spam classification include `bang`, `dollar`, `money`, `n000`, and `crl.tot_log`, all of which significantly increase the likelihood of an email being spam. In contrast, `make` has a negligible or even negative effect, suggesting it may not be a useful predictor.  

# Further Work

```{r}
set.seed(123)
index <- createDataPartition(data$yesno, p = 0.7, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
```

```{r}
glm_model <- glm(yesno ~ bang + crl.tot_log + dollar+money+n000+make, data = train_data, family = binomial(link = 'logit'))

glm_pred_prob <- predict(glm_model, newdata = test_data, type = "response")
glm_pred_class <- ifelse(glm_pred_prob > 0.5,'y','n')
glm_confusion <- confusionMatrix(factor(glm_pred_class), factor(test_data$yesno))
glm_roc <- roc(test_data$yesno, glm_pred_prob)
```

```{r}
set.seed(123)
rf_model <- randomForest(yesno ~ bang + crl.tot_log + dollar+money+n000+make, data = train_data, ntree = 500, importance = TRUE)

rf_pred_prob <- predict(rf_model, newdata = test_data, type = "prob")[, 2]
rf_pred_class <- ifelse(rf_pred_prob > 0.5,'y','n')

rf_confusion <- confusionMatrix(factor(rf_pred_class), factor(test_data$yesno))
rf_roc <- roc(test_data$yesno, rf_pred_prob)
varImpPlot(rf_model, main="Predicting")
```

```{r}
get_model_metrics <- function(model_name, confusion,k) {
  data.frame(
    Model = model_name,
    Accuracy = confusion$overall["Accuracy"],
    Sensitivity = confusion$byClass["Sensitivity"],
    Specificity = confusion$byClass["Specificity"],
    Precision = confusion$byClass["Precision"],
    AUC=as.numeric(k$auc),
    stringsAsFactors = FALSE
  )
}
results <- bind_rows(
  get_model_metrics("Random Forest", 
                   confusion = rf_confusion,k=rf_roc),
  get_model_metrics("GLM",
                   confusion = glm_confusion,k=glm_roc)
) %>% 
  mutate(across(-Model, ~ round(., 3)))

knitr::kable(results, align = "c")
```

```{r}
plot(glm_roc, col = "blue", main = "ROC Curve")
lines(rf_roc, col = "red")
legend("bottomright", legend = c("GLM", "randomForest"), col = c("blue", "red"), lwd = 2)
```
